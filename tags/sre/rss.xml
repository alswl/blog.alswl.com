<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Sre on Log4D</title>
    <link>https://blog.alswl.com/tags/sre/</link>
    <description>Recent content in Sre on Log4D</description>
    <generator>Hugo -- 0.125.6</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 11 Jun 2021 17:53:00 +0800</lastBuildDate>
    <atom:link href="https://blog.alswl.com/tags/sre/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>如何做好 PRR（Production Rediness Review）？</title>
      <link>https://blog.alswl.com/2021/06/prr/</link>
      <pubDate>Fri, 11 Jun 2021 17:53:00 +0800</pubDate>
      <guid>https://blog.alswl.com/2021/06/prr/</guid>
      <description>image from pixabay.com SRE 工程师往往会负责一个具体组件，有时也称为服务或系统（下文都称之为组件）。 需要关注的有这个组件生命周期各类事项：运行状态、日常迭代、变更计划，以及在大促等活动中的筹备、预案等等， 有些组件是团队已经在长期持续维护着的，而有些则是要去新接入。 那么，当 SRE 接手（on-borading）这样组件时， 需要做哪些事项呢， 如何将「接手」这个行为做得有掌控力、顺畅且体面？ 了解组件现状 第一步永远是了解现状，孙子</description>
      <content:encoded><![CDATA[<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/202106/prr.png" alt="prr"  />



<small>image from pixabay.com</small></p>
<p>SRE 工程师往往会负责一个具体组件，有时也称为服务或系统（下文都称之为组件）。
需要关注的有这个组件生命周期各类事项：运行状态、日常迭代、变更计划，以及在大促等活动中的筹备、预案等等，
有些组件是团队已经在长期持续维护着的，而有些则是要去新接入。
那么，当 SRE 接手（on-borading）这样组件时， 需要做哪些事项呢，
如何将「接手」这个行为做得有掌控力、顺畅且体面？</p>
<h2 id="了解组件现状">了解组件现状</h2>
<p><strong>第一步永远是了解现状</strong>，孙子兵法谋攻篇说，知己知彼，百战不殆。
现状包含组件的当前运行状态、环境，
还包含当前 SRE 团队的能力、平台是否可以顺利衔接。</p>
<!-- more -->
<p>了解一个组件，可以先以<strong>用户角度进行切入</strong>。
去理解这个组件提供什么功能，服务对象是谁，服务的规模如何？
能否将组件进行归类？是属于普通业务系统，还是基础设施？
如果时间充裕的话，还可以跟这个组件的用户进行几次沟通，咨询，他们关于这个组件使用上的痛点。</p>
<p><strong>近期和长期的规划</strong>也是需要重点关注的内容。
在组件设计和规划上面有没有什么计划和目标。根据规划我们可以推断出该组件处于何种生命周期。
生命早期的组件要多关注变更和基础能力建设；
成熟期组件往往承担了较为重要的角色，很可能承担了相当的生产流量，这时候变更、可观测性和应急方面就要花更多精力。
生命周期末期的组件则关注点是在维稳，优先考虑如何找到人，并且尽量低成本复用现有能力平台，甚至还要适当关注服务迁移和下线计划。</p>
<p><strong>第二步是以技术的视角来切入</strong>，
分别从架构、依赖、部署、可伸缩能力、容量等角度切入，具体需要回答的问题如下：</p>
<ul>
<li>架构和组件依赖：系统架构设计，功能特性图，模块切分图，核心场景的数据链路图，核心模型图；存储视角的模型图；
历史上架构是否发生变化，驱动变化的原因是什么，有哪些决策变量，这些决策变量未来是否还会继续变化？</li>
<li>部署结构：物理部署图，有没有考虑异地机房问题？上下游依赖，哪些是强依赖和弱依赖</li>
<li>Scalable：组件是否是无状态的，是否具备水平扩容能力？如果是有状态的，状态管理基于什么存储系统？流量峰值如何应对？</li>
<li>容量：当前吞吐水平如何；是否具备大流量下限流能力；流量峰值来时，是否有足够资源扩容；是否具备限流、降级能力？</li>
</ul>
<p>结合 SRE 团队服务的其他组件，还要思考一下有哪些其他组件和当前组件类型一样，有什么差异点？
有没有特殊的部署要求？</p>
<p><strong>对一个组件需要了解到什么程度才能接手？</strong> 这里我用几种程度来描述掌控力：</p>
<ul>
<li>L1 具备作为普通用户的使用能力，应急时能够定位出问题方向，找到合适的人。</li>
<li>L2 具备资深用户能力，有一定调优能力（Tuning）能力。</li>
<li>L3 具备处置问题（Trouble Shooting）能力。</li>
<li>L4 具备架构设计能力和前瞻性，能够给 SWE 反哺输入</li>
</ul>
<p>在经过一轮 PRR 完整流程之后，SRE 应当至少需要具备 L2 级别能力。
接管一段时间之后，随着对组件不断的了解，SRE 应当具备 L3 级别能力。</p>
<h2 id="明确日常和应急事项">明确日常和应急事项</h2>
<p>了解现状这个动作基本上是以静态的视角来看待组件。
完成之后，还要换成动态视角来看：<strong>有哪些日常操作（Operational）和紧急状态（Emergency）的操作</strong>？</p>
<p>需要关注的领域有可观测性、变更、应急预案：</p>
<ul>
<li>可观测性：当前监控系统是否具备 Metrics、Tracing、Logging 三类观测机制接入？是否具备中心化监控能力？
<ul>
<li>告警：接入哪些告警系统？是否拎出了核心 SLI（跌零因子、核心指标），是否具备基于 SLO 进行错误预算控制的能力？
告警的具体流程如何？是否具备告警抑制降噪能力？告警是否具备定位能力？历史数据是否可追溯？</li>
</ul>
</li>
<li>变更：目前基于什么工具进行变更？CI CD 流程分别是什么？发布周期是什么样子？配置变更二进制变更是否分离？是否有特性开关？
存储层面变更是否有考虑？各类变更流程是否有完成自动化？变更过程是否可以灰度？是否具备变更回滚能力。</li>
<li>应急预案：是否有突发流量处理预案；是否有限流、降级策略？整体不可用之后业务影响如何？流量处理上是否考虑过雪崩场景？</li>
</ul>
<p><strong>在应急方面，还需要去了解过去历史上出现过哪些的问题</strong>。
翻看组件的故障复盘文档，了解历史上故障过程，故障原因，对应的 Action 是否落地？
尤其注意的是要关注 Action 工作机制是阻断式、检查式还是发现式？
老故障放到当下如果要避免是依赖系统、流程机制还是人工？</p>
<p>由于 SRE 日常工作主要构成是两类：能力建设和 On-Call。
在了解日常、应急场景事项时，还需要持续思考这些日常事项和应急动作能否基于 SRE 的工具平台、能力平台完成。
按照能力模型等级：手工 -&gt; 工具 -&gt; 自动化 -&gt; 智能化来划分，当前日常、应急动作进入哪个环节了？
SRE 也可以借事修物，借接收这个环节，重新审视自己的各类工具平台，是否满足这些日常、应急动作，能否更快更强更安全更准更好用？</p>
<h2 id="明确服务范围">明确服务范围</h2>
<p>为了保证接手过程的顺畅，以及日后合作的体面，服务范围必须要在接手环节明确清楚。</p>
<p>什么是服务范围？就是接手之后工作内容的边界和日常合作模式。</p>
<p>一个最常见的边界划分是。变更：SRE 团队负责 CI / CD 环境建设，而 SWE 团队使用 CI 环境完成日常的部署。
SRE 团队则使用自行建设的 CD 系统进行变更管理。
日常和应急：在日常 Operational 事项和应急中，SRE 会按预案进行处置并保障组件回到最理想状态。
SRE 还需要建设可观测性、应急相关的技术基础设施，对组件全生命周期监控和应急处理。
SRE 最终承诺的组件对外 SLA，并将 SLA 拆解为 SLO 跟 SWE 共背。
在接受过程中很重要的一个工作就是，理清楚组件的 SLI / SLO，并且根据现状跟 SWE 团队商榷出对外承诺的 SLA。</p>
<p>除了服务范围，SRE 和 SWE 还要建设任务协作机制和沟通机制。
有没有统一的任务记录和流转平台？遇到稳定性相关的反馈如何，如何将需求转化为任务并追踪完成？
故障相关的 Action 如何追踪落地？一些基础环境变化以及业务上活动变化，是否有统一场合进行同步？
比较理想的情况是，基于任务管理系统，一个需求/缺陷从提出，到设计，到实现，到变更，SRE 都参与其中。
考虑到成本，现实执行时候可以根据精力、理解成本、重要程度、组件生命阶段进行微调。
有一个简单低成本执行方式，将服务领域组件进行划分后，每个领域派遣一个 SRE 进入对口的 SWE 团队进行追踪：
参加 SWE 团队的周会和日会，并将信息带回 SRE 团队同步。</p>
<p>谨记，<strong>划分没有统一的标准，不同团队，不同技术成熟度，不同生命周期组件都会导致不一样的边界和合作模式</strong>。</p>
<h2 id="总结">总结</h2>
<p>「接手」是管理的第一步。
在了解现状、明确日常和应急事项、明确服务范围等一系列动作之后，
相信 SRE 已经具备初步掌控力了。有了方法论，还要持续精益求精，将掌控力从 L2 进步到 L4。
想把事情给真正做好，核心是持续学习思考在对应领域的基础技能，并且持续了解客户的需求变化。
保持一专多能，成为随时可以顶上去独当一面的 SRE，这才能避免成为一个工单人。🐶</p>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/202106/sre-push-up.jpg" alt="sre-push-up"  />



<small>image from Twitter</small></p>
<p>扩展阅读：</p>
<ul>
<li><a href="https://sre.google/sre-book/evolving-sre-engagement-model/">The Evolving SRE Engagement Model</a></li>
<li><a href="https://sre.google/workbook/non-abstract-design/">Introducing Non-Abstract Large System Design</a></li>
<li><a href="https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons">How SREs at Google find the landmines in a service | Google Cloud Blog</a></li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>DevOps 和 SRE</title>
      <link>https://blog.alswl.com/2018/09/devops-and-sre/</link>
      <pubDate>Sun, 09 Sep 2018 23:21:40 +0800</pubDate>
      <guid>https://blog.alswl.com/2018/09/devops-and-sre/</guid>
      <description>最近有一位朋友和我聊职业发展方向问题，聊了不少 DevOps 和 SRE 话题。 我几年前刚接触这两个概念时也常常将之混淆，可惜当时没有人来解答我困惑。 现在这虽然已经极为流行，但是我发现我这位朋友对这两个职位还存在一些误区。 于是我给了一些见解并整理成文章以饕大众。 最常见的误区： DevOps 新概念，好高级哦 SRE 是高级版 DevOps 运维可以轻松转身 DevOps 工程师 让我一一给你讲解吧。 image via YouTube DevOps 和 SRE 定义 DevOps 是字面上 Dev 开发 / Ops 运维两者组合， 严格意义上 DevOps 如下（vi</description>
      <content:encoded><![CDATA[<p>最近有一位朋友和我聊职业发展方向问题，聊了不少 DevOps 和 SRE 话题。
我几年前刚接触这两个概念时也常常将之混淆，可惜当时没有人来解答我困惑。
现在这虽然已经极为流行，但是我发现我这位朋友对这两个职位还存在一些误区。
于是我给了一些见解并整理成文章以饕大众。</p>
<p>最常见的误区：</p>
<ul>
<li>DevOps 新概念，好高级哦</li>
<li>SRE 是高级版 DevOps</li>
<li>运维可以轻松转身 DevOps 工程师</li>
</ul>
<p>让我一一给你讲解吧。</p>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/201809/sre-and-devops.png" alt="sre-and-devops.png"  />



<small><a href="https://www.youtube.com/watch?v=uTEL8Ff1Zvk">image via YouTube</a></small></p>
<!-- more -->
<h2 id="devops-和-sre-定义">DevOps 和 SRE 定义</h2>
<p>DevOps 是字面上 Dev 开发 / Ops 运维两者组合，
严格意义上 DevOps 如下（via <a href="https://en.wikipedia.org/wiki/DevOps">DevOps - Wikipedia</a>）：</p>
<blockquote>
<p>DevOps（Development 和 Operations 的组合词）是一种重视“软件开发人员（Dev）
”和“IT 运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。</p>
</blockquote>
<p>SRE 全称是 Site Reliability Engineering，最早是由 Google 提出，并且在其工程实践中发扬光大。
他们还出了一本同名书籍「<a href="https://landing.google.com/sre/book.html">Site Reliability Engineering</a>」，
让这个理念在互联网工程师圈子里广泛传播。</p>
<p>Google 对 SRE 解释是（via <a href="https://en.wikipedia.org/wiki/Site_Reliability_Engineering">Site Reliability Engineering - Wikipedia</a>）：</p>
<blockquote>
<p>Site reliability engineering (SRE) is a discipline that incorporates aspects of
software engineering and applies that to operations whose goals are to
create ultra-scalable and highly reliable software systems.</p>
</blockquote>
<p>我将其翻译翻译为中文：</p>
<blockquote>
<p>网站稳定性工程师是致力于打造「高扩展、高可用系统」，并将其贯彻为原则的软件工程师。</p>
</blockquote>
<p>从定义来看，DevOps 是文化、运动和惯例，而 SRE 是有严格任职要求的职位。
文化是软性定义，文化有更多概念可以捏造，而 SRE 定义精准，就少了想象空间（也可能 SRE 门槛高 😄）。
按 Google 给出的说法是，SRE 工程师实践了 DevOps 文化。这个观点没错，但是国内的 DevOps 逐步独立出 DevOps 工程师，
所以在本文，我着重讨论的是 DevOps 工程师和 SRE 工程师两种职位对比。</p>
<h2 id="两者产生背景和历史">两者产生背景和历史</h2>
<p>互联网需求催生了 DevOps 。在最传统软件企业中，是只有 Dev 没有 Ops，
那时 Ops 可能还是只是技术支持人员。开发按照瀑布流：需求分析、系统设计、开发、测试、交付、运行，
传统软件发布是一个重量级操作。一旦发布，Dev 几乎不再直接操作。
80 后可能会记得 QQ 每年都会有一个大版本发布吧，QQ 2000 / 2003 / 2004 等等。
此时 Ops 不用和 Dev 直接高频接触，甚至针对一些纯离线业务，压根没有设立 Ops 这个岗位。</p>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/201809/qq-2004.png" alt="qq-2004.png"  />


</p>
<p>互联网浪潮之后，软件由传统意义上桌面软件演变为面向网站、手机应用。
这时候业务核心逻辑，比如交易，社交行为都不在用户桌面完成，而是在服务器后端完成。
这给互联网企业给予了极大操作空间：随时可以改变业务逻辑，这促进了业务快速迭代变更。
但即便这样，Dev 和 Ops 是极其分裂的两个环节。Ops 不关心代码是如何运作的，Dev 不知道代码如何运行在服务器上。</p>
<p>当业界还沉浸在可以每周发布版本喜悦中时，2009 年，Flicker 提出了每天发布 10+ 次概念，大大震撼了业界。
Flicker 提出了几个核心理念：</p>
<ul>
<li>业务快速发展，需要拥抱变更，小步快跑</li>
<li>Ops 目标不是为了网站稳定和快速，而是推动业务快速发展</li>
<li>基于自动化工具提高 Dev / Ops 联接：代码版本管理、监控</li>
<li>高效沟通：IRC / IM Robot（现在那些 ChatBot 套路，10 年前就被 Flicker 玩过了）</li>
<li>信任、透明、高效、互助的沟通文化</li>
</ul>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/201809/flicker.png" alt="flicker.png"  />


</p>
<p>原文 SlideShare 在这
<a href="https://www.slideshare.net/jallspaw/10-deploys-per-day-dev-and-ops-cooperation-at-flickr">10+ Deploys Per Day: Dev and Ops Cooperation at Flickr</a></p>
<p>真是让人难以想象，今天各种培训公司和一些知名大 V 在呼唤这些 DevOps 理念，
竟然在 2009 年一份幻灯片中就展现淋漓尽致。经典总是不过时，在尘封下闪耀着智慧光芒。
有些人将 DevOps 和运维自动化等同，这是只看到表象。
DevOps 目标是提高业务系统交付速度，并为之提供相关工具、制度和服务。
一些个人或培训机构添油加醋和衍生含义，都是围绕这 DevOps 本质而发散。</p>
<p>接下来聊聊 SRE 历史， SRE 出现要晚一些。在 2003 年时候 Google 的 Ben Treynor
招募了几个软件工程师，这个团队设立目的是帮助 Google 生产环境服务运行更稳定、健壮、可靠。
不同于中小型规模公司，Google 服务于十几亿用户服务，短暂服务不可用会带来致命后果。
因此 Google 走在了时代最前面，SRE 产生了。</p>
<p>这个职位为大规模集群服务，小型团队不需要这样职位设定（可能也招不起真正 SRE 😊）。
Google 在探索若干年之后，SRE 团队开始将自己心得体会写在线上，并在 2016 年将此书出版。</p>
<h2 id="两者的职能不同">两者的职能不同</h2>
<p>DevOps 文化，那么就没有一个具象职能要求。现在不少公司将 DevOps 职能单独抽取出来，称之为 DevOps 工程师。
那让我们看看 DevOps 工程师关心什么：DevOps 文化目的是提交交付速度， DevOps 工程师就自然会关心软件 / 服务的整个生命周期。</p>
<p>一个简单的公式：<code>速度 = 总量 / 时间</code>，添上工程行业术语，即 <code>交付速度 = （（功能特性 * 工程质量） / 交付时间） * 交付风险</code>。</p>
<p>功能特性交给产品经理和项目经理管理，DevOps 工程师需要关心剩下几个因素：工程质量 / 交付时间 / 交付风险。
DevOps 工程师职能如下：</p>
<ul>
<li>管理应用全生命周期（需求、设计、开发、QA、发布、运行）</li>
<li>关注全流程效率提升，挖掘瓶颈点并将其解决</li>
<li>自动化运维平台设计和研发工作（标准化、自动化、平台化）</li>
<li>支持运维系统，包括 虚拟化技术、资源管理技术、监控技术、网络技术</li>
</ul>
<p>SRE 关键词是「高扩展性」「高可用性」。高扩展性是指当服务用户数量暴增时，
应用系统以及支撑其服务（服务器资源、网络系统、数据库资源）可以在不调整系统结构，不强化机器本身性能
，仅仅增加实例数量方式进行扩容。高可用性是指，应用架构中任何环节出现不可用时，比如应用服务、网关、数据库
等系统挂掉，整个系统可以在可预见时间内恢复并重新提供服务。当然，既然是「高」可用，
那么这个时间一般期望在分钟级别。SRE 职能可以概括为以下：</p>
<ul>
<li>为 应用、中间件、基础设施等提供 选型、设计、开发、容量规划、调优、故障处理</li>
<li>为业务系统提供基于可用性、可扩展性考虑决策，参与业务系统设计和实施</li>
<li>定位、处理、管理故障，优化导致故障发生相关部件</li>
<li>提高各部件资源利用率</li>
</ul>
<h2 id="工作内容不同">工作内容不同</h2>
<p>职责不同导致两个职位工作内容也不尽相同，我将 DevOps 工程师和 SRE 工程师职能列举如下：</p>
<ul>
<li>DevOps
<ul>
<li>设定应用生命管理周期制度，扭转流程</li>
<li>开发、管理 开发工程师 /QA 工程师使用 开发平台系统</li>
<li>开发、管理 发布系统</li>
<li>开发、选型、管理 监控、报警系统</li>
<li>开发、管理 权限系统</li>
<li>开发、选型、管理 CMBD</li>
<li>管理变更</li>
<li>管理故障</li>
</ul>
</li>
<li>SRE
<ul>
<li>管理变更</li>
<li>管理故障</li>
<li>制定 SLA 服务标准</li>
<li>开发、选型、管理 各类中间件</li>
<li>开发、管理 分布式监控系统</li>
<li>开发、管理 分布式追踪系统</li>
<li>开发、管理 性能监控、探测系统（dtrace、火焰图）</li>
<li>开发、选型、培训 性能调优工具</li>
</ul>
</li>
</ul>
<p>很有趣的对比，DevOps 和 SRE 都会关心应用生命周期，特别是生命周期里面中变更和故障。
但是 DevOps 工作内容是主要为开发链路服务，一个 DevOps Team 通常会提供一串工具链，
这其中会包括：开发工具、版本管理工具、CI 持续交付工具、CD 持续发布工具、报警工具、故障处理。
而 SRE Team 则关注更为关注变更、故障、性能、容量相关问题，会涉及具体业务，产出工具链会有：
容量测量工具、Logging 日志工具、Tracing 调用链路跟踪工具、Metrics 性能度量工具、监控报警工具等。</p>
<h2 id="devops-和-sre-关系">DevOps 和 SRE 关系</h2>
<p>DevOps 首先是一种文化，后期逐渐独立成一个职位；SRE 一开始就明确是一个职位；
不少同学把 DevOps 和 SRE 搞混，是被两者表象锁迷惑，看上去这两者都有的工具属性、自动化要求也相似。
甚至有一些开发同学把这类运维工作都统一理解为：服务器 + 工具 + 自动化。这是盲人摸象，管中窥豹。</p>
<p>从技能上来说，两者都需要较强的运维技能。
在职业发展天花板上，DevOps 可能缺乏 SRE 在一些专业领域的技能：
计算机体系结构能力；高吞吐高并发优化能力；可扩展系统设计能力；复杂系统设计能力；业务系统排查能力。
两者都需要软实力，但是 SRE 面临复杂度更高，挑战更大，要求也更高：</p>
<ul>
<li>分析问题、解决问题能力</li>
<li>对业务系统更了解</li>
<li>具备高并发、高可用系统设计实施经验</li>
<li>对整个系统链路有更全面认识</li>
</ul>
<p>DevOps 具有普遍意义，现代互联网公司都需要 DevOps，但是并非所有团队对高可用性、高扩展性存在需求，它们不需要 SRE。
DevOps 工程师掌握相关技能之后，也有机会可以发展为 SRE 工程师。
而一位合格 SRE 工程师，在有选择情况下面，我相信不会去转型为 DevOps 工程师。</p>
<p>从专业背景来看，无论是 DevOps 还是 SRE 工程师，都需要研发背景，前者需要开发工具链，后者需要有较强架构设计经验。
如果有运维工程师想转型成为 DevOps 或者 SRE，那么需要补上相关技术知识。
毕竟，不是会搭建一套 Jenkins + Kubernetes 就可以自称为 DevOps / SRE 工程师。</p>
<p>怎么样，有没有解开这几个常见误区呢？希望你看到这里可以豁然开朗，最后附上两个工程师的技能点，
期望有志成为这两种工程师的同学，加油努力。</p>
<h2 id="附录技能点">附录：技能点</h2>
<p>DevOps：</p>
<ul>
<li>Operator 技能
<ul>
<li>Linux Basis
<ul>
<li>基本命令操作</li>
<li>Linux FHS（Filesystem Hierarchy Standard 文件系统层次结构标准）</li>
<li>Linux 系统（差异、历史、标准、发展）</li>
</ul>
</li>
<li>脚本
<ul>
<li>Bash / Python</li>
</ul>
</li>
<li>基础服务
<ul>
<li>DHCP / NTP / DNS / SSH / iptables / LDAP / CMDB</li>
</ul>
</li>
<li>自动化工具
<ul>
<li>Fabric / Saltstack / Chef / Ansible</li>
</ul>
</li>
<li>基础监控工具
<ul>
<li>Zabbix / Nagios / Cacti</li>
</ul>
</li>
<li>虚拟化
<ul>
<li>KVM 管理 / XEN 管理 / vSphere 管理 / Docker</li>
<li>容器编排 / Mesos / Kubernetes</li>
</ul>
</li>
<li>服务
<ul>
<li>Nginx / F5 / HAProxy / LVS 负载均衡</li>
<li>常见中间件 Operate（启动、关闭、重启、扩容）</li>
</ul>
</li>
</ul>
</li>
<li>Dev
<ul>
<li>语言
<ul>
<li>Python</li>
<li>Go（可选）</li>
<li>Java（了解部署）</li>
</ul>
</li>
<li>流程和理论
<ul>
<li>Application Life Cycle</li>
<li>12 Factor</li>
<li>微服务概念、部署、生命周期</li>
<li>CI 持续集成 / Jenkins / Pipeline / Git Repo Web Hook</li>
<li>CD 持续发布系统</li>
</ul>
</li>
<li>基础设施
<ul>
<li>Git Repo / Gitlab / Github</li>
<li>Logstash / Flume 日志收集</li>
<li>配置文件管理（应用、中间件等）</li>
<li>Nexus / JFrog / Pypi 包依赖管理</li>
<li>面向 开发 / QA 开发环境管理系统</li>
<li>线上权限分配系统</li>
<li>监控报警系统</li>
<li>基于 Fabric / Saltstack / Chef / Ansible 自动化工具开发</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>SRE：</p>
<ul>
<li>语言和工程实现
<ul>
<li>深入理解开发语言（假设是 Java）
<ul>
<li>业务部门使用开发框架</li>
<li>并发、多线程和锁</li>
<li>资源模型理解：网络、内存、CPU</li>
<li>故障处理能力（分析瓶颈、熟悉相关工具、还原现场、提供方案）</li>
</ul>
</li>
<li>常见业务设计方案和陷阱（比如 Business Modeling，N+1、远程调用、不合理 DB 结构）</li>
<li>MySQL / Mongo OLTP 类型查询优化</li>
<li>多种并发模型，以及相关 Scalable 设计</li>
</ul>
</li>
<li>问题定位工具
<ul>
<li>容量管理</li>
<li>Tracing 链路追踪</li>
<li>Metrics 度量工具</li>
<li>Logging 日志系统</li>
</ul>
</li>
<li>运维架构能力
<ul>
<li>Linux 精通，理解 Linux 负载模型，资源模型</li>
<li>熟悉常规中间件（MySQL Nginx Redis Mongo ZooKeeper 等），能够调优</li>
<li>Linux 网络调优，网络 IO 模型以及在语言里面实现</li>
<li>资源编排系统（Mesos / Kubernetes）</li>
</ul>
</li>
<li>理论
<ul>
<li>容量规划方案</li>
<li>熟悉分布式理论（Paxos / Raft / BigTable / MapReduce / Spanner 等），能够为场景决策合适方案</li>
<li>性能模型（比如 Pxx 理解、Metrics、Dapper）</li>
<li>资源模型（比如 Queuing Theory、负载方案、雪崩问题）</li>
<li>资源编排系统（Mesos / Kurbernetes）</li>
</ul>
</li>
</ul>
<h2 id="ref">Ref</h2>
<ul>
<li><a href="https://zh.wikipedia.org/wiki/DevOps">DevOps - 维基百科，自由的百科全书</a></li>
<li><a href="https://en.wikipedia.org/wiki/Site_reliability_engineering">Site reliability engineering - Wikipedia</a></li>
<li><a href="http://skill-map.stuq.org/">StuQ 技能图谱</a></li>
<li><a href="https://12factor.net/zh_cn/">The Twelve-Factor App （简体中文）</a></li>
<li><a href="https://landing.google.com/sre/book/chapters/communication-and-collaboration.html">Google - Site Reliability Engineering</a></li>
<li><a href="https://www.youtube.com/watch?v=uTEL8Ff1Zvk">What&rsquo;s the Difference Between DevOps and SRE? - YouTube</a></li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>搞定暴涨的流量</title>
      <link>https://blog.alswl.com/2016/06/capacity-planning/</link>
      <pubDate>Sun, 19 Jun 2016 23:57:39 +0800</pubDate>
      <guid>https://blog.alswl.com/2016/06/capacity-planning/</guid>
      <description>2013 年左右，我司业务发展迅速，每天晚上都会面临服务器濒临崩溃情况。 我相信每个高速发展的互联网企业在某个阶段都会面临这样的情形，比如去年爆红的「足迹」。 过程往往是：线上出现故障，手机会收到报警，然后登录到服务器上去解决问题。 处理这种问题工种现在有一个时髦的名称，叫做「SRE（Site Reliability Engineer）」系统可用性工程师。 虽然我常常救火，但是我还是想尽可能避免线上发生故障。「最好的消息，就是没有消息。</description>
      <content:encoded><![CDATA[<p>2013 年左右，我司业务发展迅速，每天晚上都会面临服务器濒临崩溃情况。
我相信每个高速发展的互联网企业在某个阶段都会面临这样的情形，比如去年爆红的「足迹」。
过程往往是：线上出现故障，手机会收到报警，然后登录到服务器上去解决问题。
处理这种问题工种现在有一个时髦的名称，叫做「SRE（Site Reliability Engineer）」系统可用性工程师。</p>
<p>虽然我常常救火，但是我还是想尽可能避免线上发生故障。「最好的消息，就是没有消息。」
减少故障出现概率，增强系统可用性，降低故障处理时间是 SRE 的最大课题。
在这里有最常用的两个手段，一个是优化性能，一个是做好容量规划和扩展。
这里我着重讨论后者「容量规划」。</p>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/201606/message.png" alt="看我的一堆报警消息"  />


</p>
<p>^ 看我的一堆报警消息</p>
<!-- more -->
<h2 id="面临的问题">面临的问题</h2>
<p>面对暴涨流量，一边是业务方的满心欢喜，一边就是工程师的烦恼和压力了。
也许是一个受欢迎的功能上线了，或者是某个社会活动导致流量爆发，系统开始出现高延迟，磁盘 IO 不够用了。
也许是 DB 第一个倒下，也许是 RPC 系统第一个倒下……
呃，大神可能会说，我艹，RPC 系统第一个倒下还搞个屁啊，赶紧倒闭算了。</p>
<p>核心的问题就是，在现有性能下面，在面临可能的大流量冲击时候，如何做到不慌不忙，能够 handle 住突如其来的流量？</p>
<h2 id="设定容量目标">设定容量目标</h2>
<p>在解决这个问题之前，我们得先考虑清楚，我们到底要多强的流量处理能力。
如果今天我们只是一个两三台服务器的小团队，却企图设计一个能够抗住 1 亿 pv 访问的系统，
显然是不现实的，至少是不经济的。</p>
<p>衡量系统容量的指标可以简化为在什么流量下面，提供什么样的可用性保证。
一个实际的样例是，在 1 亿 pv 下面，提供 99.99% 的可用性，
其可用性的评判标准是「服务器在 200ms 内返回正确的数据」。</p>
<p>这里有一个重要的概念，可用性保证，术语是服务等级协议（SLA）。
这个指标可以从大部分标准云供应商的标准条款里看到，比如我司机房供应商提供的可用性保证是 99.9%。
阿里云 ECS 的 SLA 是「99.95%」，统计周期是 1 个月
（如果故障时间低于 5 min，不计入故障时间，云供应商都这样，特别霸权）。</p>
<p>一个对 SLA 的直观认识是（具体数据来自 <a href="https://en.wikipedia.org/wiki/High_availability#Percentage_calculation">High availability - Wikipedia, the free encyclopedia</a>）：</p>
<ul>
<li>99.0% 意味着一年有 87 天不可用</li>
<li>99.5% 意味着一年 1.83 天不可用</li>
<li>99.9% 意味着一年 8.76 小时不可用</li>
<li>99.99% 意味着一年 52.56 分钟不可用</li>
<li>99.999% 意味着一年 5 分 15 秒不可用，这是高可用的一般标准</li>
</ul>
<p>设定越高的 SLA 的成本越高，具体 SLA 的设定是成本、收益、期望的平衡。
不同的业务需要的 SLA 也不一样，一般认为 99.9% 基本可用，99.99% 可用性较高，
99.999% 为高可用。</p>
<p>有些云供应商号称 8 个 9，9 个 9，那往往都是对于存储服务里面的数据不丢失这个指标。
除了忽悠忽悠人，这个 SLA 没什么用的。</p>
<h2 id="测量">测量</h2>
<p>做一件伟大事情时候，先有目标，下一步如果是迈出脚步出去闯荡，那么往往换来的是一个身心疲惫的自己。
更稳当的做法是，先摸摸清楚，自己有几把刷子，是不是还要再练练，有没有资格上战场。
没有 Profiling，就是瞎子，根本不用谈优化和容量规划。</p>
<p>对于一般的业务场景而言，常见的测量指标分为三类：</p>
<ul>
<li>服务器的硬件指标（CPU、内存、硬盘 IO、硬盘容量、网络）</li>
<li>服务的软件指标（QPS / latency / pool）</li>
<li>业务的数据指标（核心业务指标，比如注册数，核心动作次数）</li>
</ul>
<p>我司的实践情况是这样的，我们使用 Zabbix 测量服务器，用自己设计的系统收集服务数据，使用 Grafana 呈现。
后者被设计到 RPC 系统内部，数据是全量收集。
我司在业务层面的数据监控做的还不足，这种不足不仅仅体现在数据的全面性上面，还体现在相关成员（比如产品汪）对数据的利用率上面。</p>
<p>除了测量线上的实施数据，了解某个设施的性能极限也是很重要，目前常见的测量方式是：</p>
<ul>
<li>模拟流量进行测试</li>
<li>在线上进行测试，并实时跟踪进展情况，出现异常时候，停止流量切入</li>
<li>从线上引入流量到测试环境进行测试</li>
</ul>
<p>我发现，第一种方法往往不准，第三种方法对于小团队来说，成本太高。第二种方法是最粗暴和有效的。</p>
<h2 id="预警和提醒">预警和提醒</h2>
<p>仅仅知道当前系统的性能表现是不足的，重要的如何将这些数据利用起来，对未来系统增长进行预估。
流量增长 vs 资源消耗，这个曲线大部分情况是线性的，有些情况确实指数增长的。</p>
<p>常见的做法是，给核心指标设置一个阈值（比如 80% 磁盘使用率，40% 磁盘 IO 利用率），当监控的数据到达这个阈值时候。
就必须进行容量扩充，进行负载均衡。</p>
<p>一个从运维同学身上学到的是，提前采购一些设备放到机房里面，比如硬盘、内存，别到时候供应商来不及供货。
必要库存可以降低 MTBF。</p>
<p>除了设定阈值报警，应当定期跑一些脚本获得数据。定期检查报警系统，避免报警系统失效。</p>
<h2 id="必选项---scalable">必选项 - Scalable</h2>
<p>上文写到，「必要时候进行容量扩充，进行负载均衡」。
这点的提出，意味这需要<strong>保证基础设施是可扩展的，支持负载均衡，支持硬件扩容</strong>。</p>
<p>Web 系统比较容易做到横向扩容，使用 Nginx / LVS 等负载均衡即可。
中间件服务一般也是在设计时候就考虑了扩展。（什么？你们家 RPC 系统设计调用不支持扩展？什么脑残设计？！）</p>
<p>DB 级别的服务，往往就要花一些心思了，一些技术（比如 MySQL）想要做到横向扩展，
需要进行提前设计。而一些设施虽然容易进行扩展，比如 ES / Kafka 等现代化设施，
但在部署的时候仍然要进行一些提前准备。</p>
<p>除了提前做好 Scalable，还有几个和部署相关的 tips 可以供参考：</p>
<ul>
<li>使用工具：自动化部署，现在有太多工具可以供选择，比如 ansible 就是一个很好的工具</li>
<li>automatic everything：避免登录服务器操作才能保证未来自动化</li>
<li>工程化：用最佳实践去维护部署系统，用工程化的态度去写部署代码</li>
<li>保持同质，避免花样：避免使用 shell 级别的操作原语操作部署系统，使用预设的 module 去操作</li>
</ul>
<h2 id="end">End</h2>
<p>好了，现在去预测一下当大流量来临之际，你的服务会在哪些环节失败。
想不出来的话，就一点点去测量各个环节性能，然后做一把容量规划吧。</p>
<p>调优和增加容量，这是两个手段，这两个手段互相作用，互相影响。使用时候需要根据成本和收益进行选择。</p>
<p>关于容量规划的更多细节，可以看看 <a href="https://book.douban.com/subject/4200645/">Web容量规划的艺术 (豆瓣)</a>
这里看看。只是这本书写在 2010 年，并且作者介绍的过于传统运维视角一些。</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
