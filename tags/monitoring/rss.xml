<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Monitoring on Log4D</title>
    <link>https://blog.alswl.com/tags/monitoring/</link>
    <description>Recent content in Monitoring on Log4D</description>
    <generator>Hugo -- 0.125.3</generator>
    <language>zh</language>
    <lastBuildDate>Fri, 11 Jun 2021 17:53:00 +0800</lastBuildDate>
    <atom:link href="https://blog.alswl.com/tags/monitoring/rss.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>如何做好 PRR（Production Rediness Review）？</title>
      <link>https://blog.alswl.com/2021/06/prr/</link>
      <pubDate>Fri, 11 Jun 2021 17:53:00 +0800</pubDate>
      <guid>https://blog.alswl.com/2021/06/prr/</guid>
      <description>image from pixabay.com SRE 工程师往往会负责一个具体组件，有时也称为服务或系统（下文都称之为组件）。 需要关注的有这个组件生命周期各类事项：运行状态、日常迭代、变更计划，以及在大促等活动中的筹备、预案等等， 有些组件是团队已经在长期持续维护着的，而有些则是要去新接入。 那么，当 SRE 接手（on-borading）这样组件时， 需要做哪些事项呢， 如何将「接手」这个行为做得有掌控力、顺畅且体面？ 了解组件现状 第一步永远是了解现状，孙子</description>
      <content:encoded><![CDATA[<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/202106/prr.png" alt="prr"  />



<small>image from pixabay.com</small></p>
<p>SRE 工程师往往会负责一个具体组件，有时也称为服务或系统（下文都称之为组件）。
需要关注的有这个组件生命周期各类事项：运行状态、日常迭代、变更计划，以及在大促等活动中的筹备、预案等等，
有些组件是团队已经在长期持续维护着的，而有些则是要去新接入。
那么，当 SRE 接手（on-borading）这样组件时， 需要做哪些事项呢，
如何将「接手」这个行为做得有掌控力、顺畅且体面？</p>
<h2 id="了解组件现状">了解组件现状</h2>
<p><strong>第一步永远是了解现状</strong>，孙子兵法谋攻篇说，知己知彼，百战不殆。
现状包含组件的当前运行状态、环境，
还包含当前 SRE 团队的能力、平台是否可以顺利衔接。</p>
<!-- more -->
<p>了解一个组件，可以先以<strong>用户角度进行切入</strong>。
去理解这个组件提供什么功能，服务对象是谁，服务的规模如何？
能否将组件进行归类？是属于普通业务系统，还是基础设施？
如果时间充裕的话，还可以跟这个组件的用户进行几次沟通，咨询，他们关于这个组件使用上的痛点。</p>
<p><strong>近期和长期的规划</strong>也是需要重点关注的内容。
在组件设计和规划上面有没有什么计划和目标。根据规划我们可以推断出该组件处于何种生命周期。
生命早期的组件要多关注变更和基础能力建设；
成熟期组件往往承担了较为重要的角色，很可能承担了相当的生产流量，这时候变更、可观测性和应急方面就要花更多精力。
生命周期末期的组件则关注点是在维稳，优先考虑如何找到人，并且尽量低成本复用现有能力平台，甚至还要适当关注服务迁移和下线计划。</p>
<p><strong>第二步是以技术的视角来切入</strong>，
分别从架构、依赖、部署、可伸缩能力、容量等角度切入，具体需要回答的问题如下：</p>
<ul>
<li>架构和组件依赖：系统架构设计，功能特性图，模块切分图，核心场景的数据链路图，核心模型图；存储视角的模型图；
历史上架构是否发生变化，驱动变化的原因是什么，有哪些决策变量，这些决策变量未来是否还会继续变化？</li>
<li>部署结构：物理部署图，有没有考虑异地机房问题？上下游依赖，哪些是强依赖和弱依赖</li>
<li>Scalable：组件是否是无状态的，是否具备水平扩容能力？如果是有状态的，状态管理基于什么存储系统？流量峰值如何应对？</li>
<li>容量：当前吞吐水平如何；是否具备大流量下限流能力；流量峰值来时，是否有足够资源扩容；是否具备限流、降级能力？</li>
</ul>
<p>结合 SRE 团队服务的其他组件，还要思考一下有哪些其他组件和当前组件类型一样，有什么差异点？
有没有特殊的部署要求？</p>
<p><strong>对一个组件需要了解到什么程度才能接手？</strong> 这里我用几种程度来描述掌控力：</p>
<ul>
<li>L1 具备作为普通用户的使用能力，应急时能够定位出问题方向，找到合适的人。</li>
<li>L2 具备资深用户能力，有一定调优能力（Tuning）能力。</li>
<li>L3 具备处置问题（Trouble Shooting）能力。</li>
<li>L4 具备架构设计能力和前瞻性，能够给 SWE 反哺输入</li>
</ul>
<p>在经过一轮 PRR 完整流程之后，SRE 应当至少需要具备 L2 级别能力。
接管一段时间之后，随着对组件不断的了解，SRE 应当具备 L3 级别能力。</p>
<h2 id="明确日常和应急事项">明确日常和应急事项</h2>
<p>了解现状这个动作基本上是以静态的视角来看待组件。
完成之后，还要换成动态视角来看：<strong>有哪些日常操作（Operational）和紧急状态（Emergency）的操作</strong>？</p>
<p>需要关注的领域有可观测性、变更、应急预案：</p>
<ul>
<li>可观测性：当前监控系统是否具备 Metrics、Tracing、Logging 三类观测机制接入？是否具备中心化监控能力？
<ul>
<li>告警：接入哪些告警系统？是否拎出了核心 SLI（跌零因子、核心指标），是否具备基于 SLO 进行错误预算控制的能力？
告警的具体流程如何？是否具备告警抑制降噪能力？告警是否具备定位能力？历史数据是否可追溯？</li>
</ul>
</li>
<li>变更：目前基于什么工具进行变更？CI CD 流程分别是什么？发布周期是什么样子？配置变更二进制变更是否分离？是否有特性开关？
存储层面变更是否有考虑？各类变更流程是否有完成自动化？变更过程是否可以灰度？是否具备变更回滚能力。</li>
<li>应急预案：是否有突发流量处理预案；是否有限流、降级策略？整体不可用之后业务影响如何？流量处理上是否考虑过雪崩场景？</li>
</ul>
<p><strong>在应急方面，还需要去了解过去历史上出现过哪些的问题</strong>。
翻看组件的故障复盘文档，了解历史上故障过程，故障原因，对应的 Action 是否落地？
尤其注意的是要关注 Action 工作机制是阻断式、检查式还是发现式？
老故障放到当下如果要避免是依赖系统、流程机制还是人工？</p>
<p>由于 SRE 日常工作主要构成是两类：能力建设和 On-Call。
在了解日常、应急场景事项时，还需要持续思考这些日常事项和应急动作能否基于 SRE 的工具平台、能力平台完成。
按照能力模型等级：手工 -&gt; 工具 -&gt; 自动化 -&gt; 智能化来划分，当前日常、应急动作进入哪个环节了？
SRE 也可以借事修物，借接收这个环节，重新审视自己的各类工具平台，是否满足这些日常、应急动作，能否更快更强更安全更准更好用？</p>
<h2 id="明确服务范围">明确服务范围</h2>
<p>为了保证接手过程的顺畅，以及日后合作的体面，服务范围必须要在接手环节明确清楚。</p>
<p>什么是服务范围？就是接手之后工作内容的边界和日常合作模式。</p>
<p>一个最常见的边界划分是。变更：SRE 团队负责 CI / CD 环境建设，而 SWE 团队使用 CI 环境完成日常的部署。
SRE 团队则使用自行建设的 CD 系统进行变更管理。
日常和应急：在日常 Operational 事项和应急中，SRE 会按预案进行处置并保障组件回到最理想状态。
SRE 还需要建设可观测性、应急相关的技术基础设施，对组件全生命周期监控和应急处理。
SRE 最终承诺的组件对外 SLA，并将 SLA 拆解为 SLO 跟 SWE 共背。
在接受过程中很重要的一个工作就是，理清楚组件的 SLI / SLO，并且根据现状跟 SWE 团队商榷出对外承诺的 SLA。</p>
<p>除了服务范围，SRE 和 SWE 还要建设任务协作机制和沟通机制。
有没有统一的任务记录和流转平台？遇到稳定性相关的反馈如何，如何将需求转化为任务并追踪完成？
故障相关的 Action 如何追踪落地？一些基础环境变化以及业务上活动变化，是否有统一场合进行同步？
比较理想的情况是，基于任务管理系统，一个需求/缺陷从提出，到设计，到实现，到变更，SRE 都参与其中。
考虑到成本，现实执行时候可以根据精力、理解成本、重要程度、组件生命阶段进行微调。
有一个简单低成本执行方式，将服务领域组件进行划分后，每个领域派遣一个 SRE 进入对口的 SWE 团队进行追踪：
参加 SWE 团队的周会和日会，并将信息带回 SRE 团队同步。</p>
<p>谨记，<strong>划分没有统一的标准，不同团队，不同技术成熟度，不同生命周期组件都会导致不一样的边界和合作模式</strong>。</p>
<h2 id="总结">总结</h2>
<p>「接手」是管理的第一步。
在了解现状、明确日常和应急事项、明确服务范围等一系列动作之后，
相信 SRE 已经具备初步掌控力了。有了方法论，还要持续精益求精，将掌控力从 L2 进步到 L4。
想把事情给真正做好，核心是持续学习思考在对应领域的基础技能，并且持续了解客户的需求变化。
保持一专多能，成为随时可以顶上去独当一面的 SRE，这才能避免成为一个工单人。🐶</p>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/202106/sre-push-up.jpg" alt="sre-push-up"  />



<small>image from Twitter</small></p>
<p>扩展阅读：</p>
<ul>
<li><a href="https://sre.google/sre-book/evolving-sre-engagement-model/">The Evolving SRE Engagement Model</a></li>
<li><a href="https://sre.google/workbook/non-abstract-design/">Introducing Non-Abstract Large System Design</a></li>
<li><a href="https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons">How SREs at Google find the landmines in a service | Google Cloud Blog</a></li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>当我们在聊监控，我们在聊什么？</title>
      <link>https://blog.alswl.com/2017/06/monitoring-introducing/</link>
      <pubDate>Thu, 08 Jun 2017 15:40:41 +0800</pubDate>
      <guid>https://blog.alswl.com/2017/06/monitoring-introducing/</guid>
      <description>最近在团队中给大家做了一个分享，泛泛地聊了一些有关「监控」的话题。 其实做分享对分享者的作用往往大于参与者。 这是一次将自己知识的梳理的过程，于是我将这次分享整理成这篇文章。 目的 🎯 我们先来聊聊，什么是「监控」，以及我们期望通过「监控」完成哪些目的？ 传统意义上的监控，是指： 通过一些手段和工具，关注运行中的硬件、软件、用户体验的关键数据，将其暴露出来。 当关键数据出现异常时候发出警告，进行人工或者自动的响应</description>
      <content:encoded><![CDATA[<p>最近在团队中给大家做了一个分享，泛泛地聊了一些有关「监控」的话题。
其实做分享对分享者的作用往往大于参与者。
这是一次将自己知识的梳理的过程，于是我将这次分享整理成这篇文章。</p>
<p>




<img loading="lazy" src="https://e25ba8-log4d-c.dijingchao.com/upload_dropbox/201706/stock-exchange.png" alt="201706/stock-exchange.png"  />


</p>
<!-- more -->
<h2 id="目的-">目的 🎯</h2>
<p>我们先来聊聊，什么是「监控」，以及我们期望通过「监控」完成哪些目的？</p>
<p>传统意义上的监控，是指：</p>
<blockquote>
<p>通过一些手段和工具，关注运行中的<strong>硬件、软件、用户体验</strong>的关键数据，将其暴露出来。
当关键数据出现异常时候发出警告，进行人工或者自动的响应。</p>
</blockquote>
<p>我们平时看到的最常见的监控系统，比如 Zabbix，提供了丰富的模板，
可以监控服务器的 Load / CPU Usage / Alive 这些常规指标。
并在出现问题时候，对其进行报警通知。
随后运维工程师们会上线进行应急操作，case by case 的处理故障。</p>
<p>我将上面的使用目的归纳为：</p>
<ul>
<li>故障发生时提供数据报警</li>
<li>提供历史数据以供分析</li>
</ul>
<p>故事到这里似乎可以结束了，可监控真的是这么简单的么？
当然没，随着时代的进步，用户对服务提出了更为严苛的要求，
同时我们也有能力进一步控制平均故障修复时间
（<a href="https://en.wikipedia.org/wiki/Mean_time_between_failures">MTBF</a>），
上述描述的做法已经不能满足我们了。</p>
<p>现在让我们切换一下视角，从传统的 OPS 的视角切换到 SRE
（<a href="https://en.wikipedia.org/wiki/Site_reliability_engineering">Site Reliability Engineering</a>）的视角。
当我们在关注网站整体的可用性时，我们会发现：
故障警报处理当然很重要，但是我们根本上想减少甚至避免 MTBF。
我们有两种手段：
一种是去除单点故障，让问题自然发生，但是不对线上造成影响；
另一种是在问题出现的早期就发现并进行及时修复。
前者是高可用范畴，后者就是我们今天关注的「监控」了。</p>
<p>监控的目的是要<strong>将灾难消灭在襁褓里；在灾难即将出现或者发生问题时，
给大家展示直接的原因</strong>。</p>
<p>那为了达成这两个目标，我们需要回到问题的本质，重新思考两个问题：</p>
<ol>
<li>监控哪些对象？</li>
<li>如何识别故障？</li>
</ol>
<h2 id="对象-">对象 🐘🐘</h2>
<p>我们说的监控对象，一般指的都是某个资源，
资源即持有某种其他方需要的某些属性的载体，包括硬件、软件。
除了资源这种类型，还有一种常见的监控对象是「体验」，即终端用户的访问感受，
这块内容我们暂时略去。</p>
<p>让我们来先看一下常见的资源：</p>
<ul>
<li>硬件
<ul>
<li>服务器</li>
<li>网络设备</li>
</ul>
</li>
<li>软件
<ul>
<li>Application</li>
<li>Infrastructure</li>
</ul>
</li>
</ul>
<p>这个分类是粗粒度的描述，为了落地地描述监控对象对象的健康状况，
我们还要进一步细化。以「服务器」为例，我们可以将其监控的内容细化为以下监控项：</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Network interface</li>
<li>Storage devices</li>
<li>Controllers</li>
</ul>
<p>如何评估这些监控项的健康状况？我们使用
<a href="https://en.wikipedia.org/wiki/Service_level_indicator">SLI（Service Level Indicator）</a>。
比如<strong>可用性</strong>就是一个最容易理解的 SLI。
这里我将资源归为两类，面向用户提供服务的资源和面向存储的资源，
以下是针对这两类资源的常见 SLI：</p>
<ul>
<li>User-facing Service
<ul>
<li>Availability</li>
<li>Latency</li>
<li>Throughput</li>
</ul>
</li>
<li>Storage System
<ul>
<li>Latency</li>
<li>Throughput</li>
<li>durability</li>
</ul>
</li>
</ul>
<p>基于 SLI 建立的数字关键指标，称之为
<a href="https://en.wikipedia.org/wiki/Service_level_objective">Service Level Objective</a>。
SLO 往往是一组数字范围，比如 CPU 负载的 SLO 可以设置为 0.0-6.0（针对 8 核 CPU）。
不同的资源、不同的业务场景，会有不一样的 SLO 设计。</p>
<p>看到这里，我们已经聊了要监控哪些指标，那么接下来我们聊聊如何用量化的思想，
帮助指标更易于识别、分析和决策。</p>
<h2 id="量化的思想-">量化的思想 🔢</h2>
<p>刚开始担任线上救火队成员时候，当有个系统出现问题时候，我经常听到这样的描述：
网站挂了、页面打不开了，CPU 出问题了，内存爆了，线程池炸了等等。
这样的表述虽然没错，但带来的可用价值太少，信息熵太低。
这样的说辞多了，就给人产生一种不靠谱，不科学的感觉。</p>
<p>那怎样才能成为科学的描述？
古希腊哲学家在思考宇宙的时候，提出了一种心智能力，
从而打开了科学的窗子，这就是 Reasonable，中文名叫理智，这成为了自然科学的基石。
使用 Reasonable 探讨意味着探讨要深入问题的本质，不停留在表象，挖掘出真正有价值的内容。</p>
<p>但是光有 Reasonable 还不够，B站粉丝建了一个微博，每天会检查
<a href="http://weibo.com/yamanasion?refer_flag=1001030201_&amp;is_hot=1">今天B站炸了吗</a>，
他只能告诉我们炸没炸，不能给工程师带来实际的用处。
在科学的发展历史上，我们可以发现在亚里士多德的著作里没有任何数据公式。
他对现象只有描述，只是定性分析，通过描述性状来阐述定理。
这个定性的研究方式到了伽利略那里才出现了突破。
这里我们可以引入第二个关键词是 Quantifier，量化。
伽利略率先使用定量分析的方法，并将其运用到动力学和天文学，从而开创了近代科学。</p>
<p>如果我们以定量的方式来描述网站挂没挂，就会变成：网站的响应耗时在 30s，基本无法使用。
描述线程池出问题，就会变成：active 线程数量是 200，已经到达 maxCount 数量，无法进行分配。
你看，通过这样的描述，我们一下子就能发现问题出在哪里。</p>
<h2 id="use-">USE 💡</h2>
<p>现在我们已经了解了「监控哪些对象？」，以及尝试用「量化」这个法宝来「识别故障」。
那有没有一些最佳实践帮助大家高效的识别故障呢？这里我推荐 Brend Gregg 大神的 <a href="http://www.brendangregg.com/usemethod.html">USE 方法</a>。
Brend Gregg 是 Netflix 的首席 SRE，著有 <a href="http://www.brendangregg.com/sysperfbook.html">Systems Performance Book</a>，
目前已经出版中文版 <a href="https://www.amazon.cn/%E5%9B%BE%E4%B9%A6/dp/B0140I5WPK">性能之巅:洞悉系统、企业与云计算</a>。</p>
<p>USE 分别是三个单词的首字母缩写：</p>
<ul>
<li>Utilization：使用率，CPU running percent，硬盘的 IO</li>
<li>Saturation：饱和度，一般偏存储型资源，内存使用，硬盘使用</li>
<li>Error：错误数</li>
</ul>
<p>我们可以为每个资源找到各自的 USE 度量指标，具体的 Check List 清单可以参考
<a href="http://www.brendangregg.com/USEmethod/use-rosetta.html">USE Method: Rosetta Stone of Performance Checklists</a>。</p>
<p>这里举个例子，前段时间在设计 MySQL HA 方案时候，同时关注了 MySQL 的监控方案，
那么针对 MySQL，我们要做哪些监控呢？下面是使用 USE 方法设计出来的 SLI：</p>
<ul>
<li>Business
<ul>
<li>Questions：语句计总，Throughput</li>
<li>Slow_queries：慢查询计总，Error</li>
<li>Com_select：查询语句计总，Throughput</li>
<li>Com_insert：插入语句计总，Throughput</li>
<li>Com_update：更新语句计总，Throughput</li>
</ul>
</li>
<li>Threads &amp; Connections
<ul>
<li>Threads_connected：当前连接数，Utilization</li>
<li>Threads_running：当前使用中连接数，Utilization</li>
<li>Aborted_connects：尝试连接失败数，Error</li>
<li>Connection_errors_max_connections：由于连接数超标从而失败的连接数，Error</li>
</ul>
</li>
<li>Buffer
<ul>
<li>Innodb_buffer_pool_pages_total：内存使用页数，Utilization</li>
<li>Innodb_buffer_pool_read_requests：读请求数计总，Utilization</li>
</ul>
</li>
</ul>
<h2 id="完-">完 🏁</h2>
<p>如果你对我上面描述的还意犹未尽，建议你可以看 <a href="https://book.douban.com/subject/19992841/">Effective Monitoring and Alerting</a>。
虽然本书没有中文版，但是关于监控、报警的原理解析很到位，值得一看。
另外还有一本 <a href="https://book.douban.com/subject/26875239/">SRE: Google运维解密</a>，
里面有不少篇幅在讲「SLA」，也是和监控、报警息息相关的。</p>
<p>这次讲了一些概念性的内容，期望对大家有帮助，下一次我再分享一篇文章，聊聊 Metrics。</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
